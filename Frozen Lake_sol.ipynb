{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen Lake\n",
    "===\n",
    "The goal of this computer assignment is to get familiar with OpenAI Gym, implement value iteration and policy iteration.\n",
    "\n",
    "Problem Description\n",
    "---\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. For more information visit https://gym.openai.com.\n",
    "\n",
    "In this computer assigment, you'll get familiar with Frozen Lake environment and implement value and policy iteration algorithms. Frozen Lake is an environment where the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. For more information please visit https://gym.openai.com/envs/FrozenLake8x8-v0/.\n",
    "\n",
    "Your Job\n",
    "---\n",
    "1. Get started with gym by following the steps here https://gym.openai.com/docs/.\n",
    "2. Read https://gym.openai.com/envs/FrozenLake8x8-v0/ to get familiar with the environment, states, reward function, etc.\n",
    "3. Implement the $\\texttt{value_iteration}$ function below.\n",
    "4. Implement the $\\texttt{policy_iteration}$ function below.\n",
    "5. Answer the questions (By double click on the cell you can edit the cell and put your answer below each question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Gamma=0.9 ----------\n",
      "Iteration step =  314\n",
      "Value iteration took 0.417464017868042 seconds.\n",
      "Average score =  0.00582952534124281\n",
      "---------- Gamma=0.95 ----------\n",
      "Iteration step =  496\n",
      "Value iteration took 0.5441701412200928 seconds.\n",
      "Average score =  0.04908442798056828\n",
      "---------- Gamma=0.99 ----------\n",
      "Iteration step =  1125\n",
      "Value iteration took 1.4153997898101807 seconds.\n",
      "Average score =  0.41152692263539387\n",
      "---------- Gamma=0.9999 ----------\n",
      "Iteration step =  2335\n",
      "Value iteration took 2.480024814605713 seconds.\n",
      "Average score =  0.8707724315584127\n",
      "---------- Gamma=1 ----------\n",
      "Iteration step =  2356\n",
      "Value iteration took 2.50634503364563 seconds.\n",
      "Average score =  0.876\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "def run_episode(env, policy, gamma, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma=gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma, epsilon=1e-20, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements value iteration algorithm for the infinite\n",
    "    horizon discounted MDPs. If the sup norm of v_k - v_{k-1} is less than\n",
    "    epsilon or number of iterations reaches max_iterations, it should return\n",
    "    the value function.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    ########################### Your Code Here ###########################\n",
    "    # Hint: see implementation of extract_policy\n",
    "    for i in range(max_iterations):\n",
    "        v1 = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            Q = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for next_sr in env.P[s][a]:\n",
    "                    p, s_, r, _ = next_sr\n",
    "                    Q[a] += (p * (r + gamma * v1[s_]))\n",
    "            v[s] = np.max(Q)\n",
    "        if(np.sum(np.fabs(v1-v)) <= epsilon):\n",
    "            print(\"Iteration step = \",i)\n",
    "            break\n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Value iteration took {0} seconds.\".format(end - start))\n",
    "    return v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1111)\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        optimal_v = value_iteration(env, gamma);\n",
    "        policy = extract_policy(optimal_v, gamma)\n",
    "        policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "        print('Average score = ', policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Gamma=0.9 ----------\n",
      "iteration step =  4\n",
      "Policy iteration took 0.17940711975097656 seconds.\n",
      "Average scores =  0.00839832646151676\n",
      "---------- Gamma=0.95 ----------\n",
      "iteration step =  2\n",
      "Policy iteration took 0.17818713188171387 seconds.\n",
      "Average scores =  0.041445836926981\n",
      "---------- Gamma=0.99 ----------\n",
      "iteration step =  7\n",
      "Policy iteration took 0.97104811668396 seconds.\n",
      "Average scores =  0.42107150393173565\n",
      "---------- Gamma=0.9999 ----------\n",
      "iteration step =  11\n",
      "Policy iteration took 3.004136085510254 seconds.\n",
      "Average scores =  0.911345348434091\n",
      "---------- Gamma=1 ----------\n",
      "iteration step =  5\n",
      "Policy iteration took 1.7235159873962402 seconds.\n",
      "Average scores =  0.84\n"
     ]
    }
   ],
   "source": [
    "def compute_policy_v(env, policy, gamma):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements policy iteration algorithm.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    ########################### Your Code Here ###########################\n",
    "    for i in range(max_iterations):\n",
    "        v1 = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(v1,gamma)\n",
    "        if(np.all(policy == new_policy)):\n",
    "            print(\"iteration step = \",i)\n",
    "            break\n",
    "        policy = new_policy\n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Policy iteration took {0} seconds.\".format(end - start))\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1111)\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        optimal_policy = policy_iteration(env, gamma=gamma)\n",
    "        scores = evaluate_policy(env, optimal_policy, gamma=gamma)\n",
    "        print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "--- \n",
    "\n",
    "#### 1. How many iterations did it take for the value iteration to converge? How about policy iteration?\n",
    "Value iteration:                         Policy iteration:\n",
    "Gamma        Iteration step              Gamma        Iteration step\n",
    "0.9          314                         0.9          4\n",
    "0.95         496                         0.95         2\n",
    "0.99         1125                        0.99         7\n",
    "0.9999       2335                        0.9999       11\n",
    "1            2356                        1            5\n",
    "\n",
    "\n",
    "#### 2. How much time did it take for the value iteration to converge? How about the policy iteration?\n",
    "Value iteration:            \n",
    "Gamma        Time(seconds)  \n",
    "0.9          0.417464017868042 seconds.            \n",
    "0.95         0.5441701412200928 seconds.            \n",
    "0.99         1.4153997898101807 seconds.           \n",
    "0.9999       2.480024814605713 seconds.           \n",
    "1            2.50634503364563 seconds.           \n",
    "\n",
    "Policy iteration:            \n",
    "Gamma        Time(seconds)  \n",
    "0.9          0.17940711975097656 seconds.            \n",
    "0.95         0.17818713188171387 seconds.            \n",
    "0.99         0.97104811668396 seconds.           \n",
    "0.9999       3.004136085510254 seconds.           \n",
    "1            1.7235159873962402 seconds.  \n",
    "\n",
    "\n",
    "#### 3. Which algorithm is faster? Why?\n",
    "Policy algorithm is faster. Since in policy iteration, the inf operation is just finding the inferior of $c(x,u)$. The other part $\\gamma\\sum_{x^{'}}P_{xx^{'}}(\\pi_{n}(x))V^{\\pi_n}(x^{'})$ is fixed in each step. However, in value iteration, we need to find the inferior of $c(x,u)+\\gamma\\sum_{x^{'}}P_{xx^{'}}(u)V_{k}(x^{'})$. It takes more time to calculate the latter function.\n",
    "\n",
    "#### 4. How does the average score change as $\\gamma$ gets closer to 1? Why?\n",
    "The average score will be increasing as the $\\gamma$ gets closer to 1, since the average score is calculated by the average of total_reward $\\sum\\gamma^kc^*(x_ku_k)$, which is a monotonic function of $\\gamma$.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
